---
title: "Can Machine Learning Techniques Predict Non-performance of Farm Non-Real Estate Loans in the Ag Finance Databook"
author: |
  | "Mindy Mallory, Todd Kuethe, and Todd Hubbs"
  | "University of Illinois"
date: "April 6, 2018"
output: beamer_presentation
---


# Outline of Talk

+ Basic motivation of why it is intersting/important to predict non-performing loans in the farm sector. If there's gonna be another 1980's, want to see it coming
+ Problem is inherently 'high dimentional' few observations, many (probably colinear) possible predictors
+ Can the shiny new toy (machine learning techniques) help with this problem?
+ Discussion of basic similarities and differences between ML and typical econometrics
+ Introduce (in spirit) the ML models test run in this talk
+ Results individually
+ Results compare across
+ Not clear what if any could be an econometric straw man? What would be the most natural traditional linear model to put these up against?


# Research Question

+ Try to predict farm non-performing loans in the Agricultural Finance Databook
  + Selected 'A' and 'B' tables
  
![](assets\kcfedsnip.png)


# Research Question

+ Try to predict farm non-performing loans in the Agricultural Finance Databook
  
## Why??

# Research Question

+ Try to predict farm non-performing loans in the Agricultural Finance Databook
  
## Why??

+ See 'the next 1980's crisis coming'
+ Even if not 1980's crisis levels $\Rightarrow$  Still good to predict what non-performance will look like next quarter.



# Research Question

+ Try to predict farm non-performing loans in the Agricultural Finance Databook

## Why??

+ Less access to bank-level data
+ Can machine learning techniques help us make sense of aggregate-level data?
+ Can machine learning techniques help us handle high-dimentional data?

+ Like in the Ag Finance Databook



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning=FALSE, message=FALSE)
```

```{r, echo = FALSE}
# Working with the [Ag Finance Databook](https://www.kansascityfed.org/research/indicatorsdata/agfinancedatabook) Some documentation here. But not a lot...
# install.packages("tidyverse")
library(tidyverse)
library(readxl)



url <- "https://www.kansascityfed.org/~/media/files/publicat/research/indicatorsdata/agfinance/historicaldata-dbk.xlsx"
destfile <- "databook.xlsx"
download.file(url, destfile, mode = 'wb')
```



```{r, echo = FALSE}
# 'a' tables

library(readxl)

data.a1  <- read_excel("databook.xlsx", sheet = "afdr_a1")
data.a2  <- read_excel("databook.xlsx", sheet = "afdr_a2")
data.a3  <- read_excel("databook.xlsx", sheet = "afdr_a3")
data.a4  <- read_excel("databook.xlsx", sheet = "afdr_a4")
data.a5  <- read_excel("databook.xlsx", sheet = "afdr_a5")
data.a6  <- read_excel("databook.xlsx", sheet = "afdr_a6")
data.a7  <- read_excel("databook.xlsx", sheet = "afdr_a7")
data.a8  <- read_excel("databook.xlsx", sheet = "afdr_a8")
data.a9  <- read_excel("databook.xlsx", sheet = "afdr_a9")
data.a10  <- read_excel("databook.xlsx", sheet = "afdr_a10")
data.a11  <- read_excel("databook.xlsx", sheet = "afdr_a11")
data.a12  <- read_excel("databook.xlsx", sheet = "afdr_a12")
data.a13  <- read_excel("databook.xlsx", sheet = "afdr_a13")
data.a14  <- read_excel("databook.xlsx", sheet = "afdr_a14")

```


```{r, echo = FALSE}
data.b1  <- read_excel("databook.xlsx", sheet = "afdr_b1")
data.b2  <- read_excel("databook.xlsx", sheet = "afdr_b2")
data.b3  <- read_excel("databook.xlsx", sheet = "afdr_b3")
data.b4  <- read_excel("databook.xlsx", sheet = "afdr_b4")
data.b5  <- read_excel("databook.xlsx", sheet = "afdr_b5")
data.b6  <- read_excel("databook.xlsx", sheet = "afdr_b6")
data.b7  <- read_excel("databook.xlsx", sheet = "afdr_b7")
data.b8  <- read_excel("databook.xlsx", sheet = "afdr_b8")
data.b9  <- read_excel("databook.xlsx", sheet = "afdr_b9")
```



```{r, echo = FALSE}
data.c1  <- read_excel("databook.xlsx", sheet = "afdr_c1")
data.c2  <- read_excel("databook.xlsx", sheet = "afdr_c2")
data.c3  <- read_excel("databook.xlsx", sheet = "afdr_c3")
data.c4  <- read_excel("databook.xlsx", sheet = "afdr_c4")
data.c5  <- read_excel("databook.xlsx", sheet = "afdr_c5")
data.c6  <- read_excel("databook.xlsx", sheet = "afdr_c6")
data.c7  <- read_excel("databook.xlsx", sheet = "afdr_c7")

```


# Historical Coverage: The 'A' Tables

Section A of the Agricultural Finance Databook are from quarterly sample surveys for non-real-estate farm loans of $1,000 or more made by commercial banks. a1-a7 'attempt to show estimates that are comparable to those that have been presented for a number of years.' 

| A table |  Range                                   |
|:-----------------------------|:---------------------------------|
|*A1-A6*    | *1977- annual and quarters*  |
|A7       | 1987- quarters (interest rates - lots of NA's before 1990's)              |
|A8-A13   | 1999Q4- quarters            |

# Historical Coverage: The 'B' Tables

Section B of the AFD are quarterly reports of condition and income for commercial banks. 

| B table |  Range                                   |
|:-----------------------------|:---------------------------------|
|*B1       | 1973- Q4, 1986-quarterly*   |
|*B2*       | *1987- quarterly*            |
|B3       | 1984- annual                     |
|B4       | 1991- annual and quarters   |
|B5       | 1991- annual and quarters  |
|*B6       | 1987- quarterly*  |
|B7       | 1988- quarterly, 1977- annual |
|B8       | 1975- quarterly  |
|B9       | 1981- quarterly  | 

# Historical Coverage: The 'C' Tables

Section C of the AFD are quarterly surveys of agricultural credit conditions and farmland values at commercial banks. 

| C table |  Range                                   |
|:-----------------------------|:---------------------------------|
|C1       | 1991-quarterly   |
|C2       | 1991-quarterly            |
|C3       | 1991-quarterly                     |
|C4       | 1991-quarterly    |
|C5       | 2001- quarterly  |
|C6       | 1991-quarterly  |
|C7       | 2001- quarterly |

# This Study: Selected Variables from the A and B Tables
A1-A6  
Feeder livestock	
Other livestock	
Other current operating expenses	
Farm machinery and equipment	
Other	
1 to 9	
10 to 24	
25 to 99	
100 and over	
Small or mid-size	
Large

# This Study: Selected Variables from the A and B Tables

A.1. *Number* of Non-Real Estate Bank Loans  
A.2. *Average Size* of Non-Real Estate Bank Loans  
A.3. *Volume *of Non-Real Estate Bank Loans  
A.4. *Average Maturit*y of Non-Real Estate Bank Loans  
A.5. *Average Effective Interest Rate* on Non-Real Estate Bank Loans
A.6. *Share* of Non-Real Estate Bank Loans *with a Floating Interest Rate*  

# This Study: Selected Variables from the A and B Tables
B1 Loan volumes

# This Study: Selected Variables from the A and B Tables
B2  
Estimated volume - Total	
Estimated volume - Past due 30 to 89 days, accruing	  
Estimated volume - Nonperforming - Total	 
Estimated volume - Nonperforming - Past due 90 days, accruing  	
Estimated volume - Nonperforming - Non-accruing	  
Share of outstanding loans - Total	  
Share of outstanding loans - Past due 30 to 89 days, accuring	  
Share of outstanding loans - Nonperforming - Total	  
Share of outstanding loans - Nonperforming - Past due 90 days, accruing	  
Share of outstanding loans - Nonperforming - Non-accruing  

# This Study: Selected Variables from the A and B Tables
B6 - Interst rates
 
# Potential independent variables

*Share of outstanding loans Non-performing total*

Share of outstanding loans Non-performing, 90 days past due, accrueing

Share of outstanding loans Non-performing, non-accrueing

# How many Lags?

1

# High Dimentional Problem 

+ 122 observations, and 88 potential 'predictors'. Many are most probably highly colinear. 

# Econometrics versus Machine Learning 

+ ML more flexible on model specification
  + Use cross validation to keep honest wrt overfitting
  
# Econometrics versus Machine Learning: Variable Selection

+ ML has variable selection baked into most procedures

+ Econometrics strategies to reduce high dimentionality
  + Theory 
  + Make and index
  + Ad hoc
  + General-to-Specific
  + Specific-to-General
  
  


```{r, echo = FALSE}
library(dplyr)
data.b6[data.b6 == "."] <- 0

table_list <- list(data.a3, data.a4, data.a5, data.a6, data.b1[,1:3], data.b2, data.b6)



tables   <- left_join(data.a1, data.a2, by = "Period")
n <- length(table_list)

for(i in 1:n){
  
  tables <- left_join(tables, table_list[[i]], by = "Period")
  
}
tables <- tables[41:dim(tables)[1],]
tables <- tables[, -86]

```



# Partial Least Squares

1. Fit OLS $Y = \beta^1 X + \epsilon^1$
2. Set $Z_1 = \beta ^1 X$
3. Fit OLS $\epsilon^1 = \beta^2 X + \epsilon^2$
4.  Set $Z_2 =  \beta^2 X$
.
.
.
5. Fit OLS $Y = \gamma Z + \eta$ 

# PLS Results

```{r, echo = FALSE}
# Partial Least Squares
library(pls)
library(xts)

# Questions: How many lags...
#detach("package:dplyr", unload = TRUE)
#tables$PeriodII  <- as.yearqtr(1977 +seq(0, dim(tables)[1]-1)/4, format = "%Y Quarter %q")
#tables  <- xts(tables, order.by = tables$PeriodII )
#tables  <- cbind(tables[5:dim(tables)[1], 83], lag(tables, n=1))

tables  <- cbind(tables[2:dim(tables)[1], 83], tables[1:(dim(tables)[1]-1), c(-2, -83)]) # dplyr was not lagging correctly so I lagged by hand and made col 83 first col


tables[, 74] <- as.numeric(tables[, 74])  # Somehow, columns got cast as character
tables[, 75] <- as.numeric(tables[, 75])  # Somehow, columns got cast as character
tables[, 87] <- as.numeric(tables[, 87])  # Somehow, columns got cast as character
tables[, 88] <- as.numeric(tables[, 88])  # Somehow, columns got cast as character
tables[, 89] <- as.numeric(tables[, 89])  # Somehow, columns got cast as character
tables[, 90] <- as.numeric(tables[, 90])  # Somehow, columns got cast as character

tables  <- tables %>% na.omit %>% as.data.frame
train   = sample(1:nrow(tables), nrow(tables)/2)
test    = tables[-train,]
pls.fit = plsr(tables[, 1]~., data = tables, subset = train, scale = FALSE, validation = "CV")
#summary(pls.fit)

validationplot(pls.fit,val.type="MSEP")
```

# PLS Results

```{r, echo = FALSE}

#pls.pred=predict(pls.fit, ncomp=2)[-train]
#mean((pls.pred-test[, 1])^2)
plot(pls.fit)
```


# Lasso Regression Results

```{r, echo = FALSE}
library(glmnet)
grid    <-10^seq(10,-2,length=100)
tables2 <- data.matrix(tables)
lasso.mod=glmnet(tables2[train , c(-1, -2 )], tables2[train, 1],alpha=1,lambda=grid)
plot(lasso.mod)
```


# Lasso Regression Results
```{r, echo = FALSE}

cv.out <- cv.glmnet(tables2[train , c(-1, -2)], tables2[train, 1], alpha = 1)
plot(cv.out)
```

# Lasso Regression Results

```{r, echo = FALSE}
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam, newx=tables2[-train , c(-1, -2)])
# mean((lasso.pred-test[, 1])^2)


out=glmnet(tables2[ , c(-1, -2)], tables2[, 1], alpha=1, lambda=grid)
coef(cv.out, s = "lambda.min")
```

# Boosting Regression Trees

```{r, echo = FALSE}
library(gbm)

boost   <- gbm(tables[train, 1]~., data = tables[train, c(-1, -2)], distribution = "gaussian", n.trees = 10000, interaction.depth = 4, shrinkage = .0001)
boost.pred <-  predict(boost, newdata = tables[-train, c(-1, -2)], n.trees = 1000)
mean((boost.pred-test[, 1])^2)
```


